import pandas as pd
import numpy as np
import xgboost as xgb
import optuna
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
import random
import joblib
import matplotlib.pyplot as plt
import time

print("--- Iniciando Etapa 1: Resumen de productos ---")
df = pd.read_csv('clean_DATOS-VENTAS.csv')

valor_unitario_mas_frec = (
    df.groupby('producto')['valor_unitario']
    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])
    .reset_index()
    .rename(columns={'valor_unitario': 'valor_unitario_mas_frec'})
)

apariciones = df['producto'].value_counts().reset_index()
apariciones.columns = ['producto', 'apariciones']

resumen = valor_unitario_mas_frec.merge(apariciones, on='producto')

resumen = resumen[['producto', 'valor_unitario_mas_frec', 'apariciones']]
print("‚úÖ Resumen de productos calculado y mantenido en memoria.")
print("\n--- Iniciando Etapa 2: Creaci√≥n de dataset completo ---")


df['fecha'] = pd.to_datetime(df['fecha'])
df_grouped = df.groupby(['producto', 'fecha']).agg(
    cantidad=('cantidad', 'sum'),
    valor_unitario=('valor_unitario', 'mean')
).reset_index()

df_grouped['cantidad_log'] = np.log1p(df_grouped['cantidad'])

productos_filtrados = resumen[resumen['apariciones'] > 275]['producto'].unique()

df_grouped['fecha'] = df_grouped['fecha'].dt.strftime('%Y-%m-%d')
fechas = pd.date_range(df_grouped['fecha'].min(), df_grouped['fecha'].max()).strftime('%Y-%m-%d')

combinaciones = pd.MultiIndex.from_product([productos_filtrados, fechas], names=['producto', 'fecha']).to_frame(index=False)

df_completo = combinaciones.merge(df_grouped, on=['producto', 'fecha'], how='left')

df_completo = df_completo.merge(resumen[['producto', 'valor_unitario_mas_frec']], on='producto', how='left')

df_completo['cantidad'] = df_completo['cantidad'].fillna(0)
df_completo['valor_unitario'] = df_completo['valor_unitario'].fillna(df_completo['valor_unitario_mas_frec'])
df_completo['cantidad_log'] = df_completo['cantidad_log'].fillna(0)

if 'dia_semana' not in df_completo or df_completo['dia_semana'].isnull().any():
    dias = {0: 'Lunes', 1: 'Martes', 2: 'Mi√©rcoles', 3: 'Jueves', 4: 'Viernes', 5: 'S√°bado', 6: 'Domingo'}
    fechas_dt = pd.to_datetime(df_completo['fecha'])
    df_completo['dia_semana'] = fechas_dt.dt.weekday.map(dias)
    def estacion(mes):
        if mes in [12, 1, 2]:
            return 'verano'
        elif mes in [3, 4, 5]:
            return 'oto√±o'
        elif mes in [6, 7, 8]:
            return 'invierno'
        else:
            return 'primavera'
    df_completo['estacion'] = fechas_dt.dt.month.map(estacion)

df_final_completo = df_completo[['producto', 'cantidad', 'valor_unitario', 'fecha', 'dia_semana', 'estacion', 'cantidad_log']]
df_final_completo = df_final_completo.sort_values(['fecha', 'producto'])
print("‚úÖ Dataset completo generado y mantenido en memoria.")


print("\n--- Iniciando Etapa 3: Entrenamiento y evaluaci√≥n del modelo ---")
random_state_used = random.randint(0, 1000000)
print(f"üå± Semilla generada para esta ejecuci√≥n: {random_state_used}")

df_entrenamiento = df_final_completo.copy()
df_entrenamiento['fecha'] = pd.to_datetime(df_entrenamiento['fecha'])
df_entrenamiento = df_entrenamiento.sort_values(['producto', 'fecha'])

fecha_corte = df_entrenamiento['fecha'].max() - pd.Timedelta(days=30)
train_df = df_entrenamiento[df_entrenamiento['fecha'] <= fecha_corte].copy()
val_df = df_entrenamiento[df_entrenamiento['fecha'] > fecha_corte].copy()

def crear_vars_lag_rolling(df, window=3):
    df = df.copy()
    df['cantidad_lag_1'] = df.groupby('producto')['cantidad'].shift(1)
    df['cantidad_rolling_mean_3'] = df.groupby('producto')['cantidad'].rolling(window=window).mean().reset_index(0, drop=True)
    df['cantidad_rolling_std_3'] = df.groupby('producto')['cantidad'].rolling(window=window).std().reset_index(0, drop=True)
    df['cantidad_log_lag_1'] = df.groupby('producto')['cantidad_log'].shift(1)
    df['cantidad_log_rolling_mean_3'] = df.groupby('producto')['cantidad_log'].rolling(window=window).mean().reset_index(0, drop=True)
    df['cantidad_log_rolling_std_3'] = df.groupby('producto')['cantidad_log'].rolling(window=window).std().reset_index(0, drop=True)
    return df

train_df = crear_vars_lag_rolling(train_df)
val_df = crear_vars_lag_rolling(val_df)

train_df.dropna(inplace=True)
val_df.dropna(inplace=True)

cat_cols = ['producto', 'dia_semana', 'estacion']
num_cols = [
    'valor_unitario',
    'cantidad_lag_1',
    'cantidad_rolling_mean_3',
    'cantidad_rolling_std_3',
    'cantidad_log_lag_1',
    'cantidad_log_rolling_mean_3',
    'cantidad_log_rolling_std_3'
]

def preparar_df(df, base_columns=None):
    X_cat = pd.get_dummies(df[cat_cols], drop_first=True)
    X_num = df[num_cols].reset_index(drop=True)
    X = pd.concat([X_num, X_cat.reset_index(drop=True)], axis=1)
    if base_columns is not None:
        X = X.reindex(columns=base_columns, fill_value=0)
    return X

X_train = preparar_df(train_df)
X_val = preparar_df(val_df, base_columns=X_train.columns)

joblib.dump(X_train.columns, 'model_columns.pkl')
print('‚úÖ Columnas del modelo guardadas como model_columns.pkl')

y_train = train_df['cantidad']
y_val = val_df['cantidad']

def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'random_state': random_state_used
    }
    model = xgb.XGBRegressor(**params)
    scores = cross_val_score(model, X_train, y_train, scoring=make_scorer(r2_score), cv=3)
    return scores.mean()

study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=random_state_used))
study.optimize(objective, n_trials=100)

print("\n‚úÖ Mejores hiperpar√°metros encontrados:")
best_params = study.best_params
best_params['random_state'] = random_state_used
print(best_params)
print(f"Mejor R¬≤ promedio en CV: {study.best_value:.4f}")
print(f"üß¨ Semilla utilizada para esta ejecuci√≥n: {random_state_used}")

best_model = xgb.XGBRegressor(**best_params)
best_model.fit(X_train, y_train)

joblib.dump(best_model, 'mejor_modelo.pkl')
print('‚úÖ Modelo guardado como mejor_modelo.pkl')

y_pred_continuo = best_model.predict(X_val)
y_pred_continuo = np.maximum(0, y_pred_continuo)
y_pred = np.round(y_pred_continuo)

val_df = val_df.copy()
val_df['prediccion'] = y_pred_continuo
val_df.insert(val_df.columns.get_loc('prediccion'), 'prediccion_redondeada', y_pred)
val_df['error_abs'] = np.abs(val_df['cantidad'] - val_df['prediccion_redondeada'])
val_df['error_rel'] = val_df['error_abs'] / (val_df['cantidad'] + 1e-5)

umbral_error_abs = val_df['error_abs'].quantile(0.95)
umbral_error_rel = 0.5

def clasificar_error(row):
    if row['error_abs'] <= 1:
        return 'Dentro del margen de ¬±1 unidad'
    elif row['error_abs'] > umbral_error_abs:
        return 'Outlier por cantidad extrema'
    elif row['error_rel'] > umbral_error_rel:
        return 'Mala predicci√≥n (Error Relativo Alto)'
    else:
        return 'Predicci√≥n aceptable'

val_df['clasificacion_error'] = val_df.apply(clasificar_error, axis=1)

print(f"\nüìä Evaluaci√≥n en Validaci√≥n:")
print(f"R¬≤ est√°ndar: {r2_score(y_val, y_pred):.3f}")
print(f"MAE: {mean_absolute_error(y_val, y_pred):.3f}")

# --- M√©tricas adicionales ---
from sklearn.metrics import mean_squared_error, median_absolute_error, explained_variance_score

# SMAPE: Error Porcentual Absoluto Medio Sim√©trico
smape = 100 * np.mean(2 * np.abs(y_pred - y_val) / (np.abs(y_val) + np.abs(y_pred) + 1e-8))
print(f"SMAPE: {smape:.2f}%")

print("\n--- Iniciando Etapa 4: Visualizaci√≥n de predicciones ---")
val_df_export = val_df[['fecha', 'producto', 'cantidad', 'prediccion_redondeada']].copy()
val_df_export.rename(columns={'prediccion_redondeada': 'predicho'}, inplace=True)

productos = val_df_export['producto'].unique()

if len(productos) > 0:
    fig, axs = plt.subplots(len(productos), 1, figsize=(14, 5 * len(productos)), sharex=True)
    if len(productos) == 1:
        axs = [axs]
    for i, producto in enumerate(productos):
        val_prod = val_df_export[val_df_export['producto'] == producto]
        axs[i].plot(val_prod['fecha'], val_prod['cantidad'], label='Cantidad real', marker='o')
        axs[i].plot(val_prod['fecha'], val_prod['predicho'], label='Cantidad predicha', marker='o')
        axs[i].set_title(f'{producto}')
        axs[i].set_ylabel('Cantidad')
        axs[i].legend()
        axs[i].grid(True)
    plt.xlabel('Fecha')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('predicciones_grafico.png') 
    plt.close(fig) 
    print("‚úÖ Gr√°fico de predicciones guardado como 'predicciones_grafico.png'.")
else:
    print("‚ÑπÔ∏è No hay datos de validaci√≥n para graficar.")
